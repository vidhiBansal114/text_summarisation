{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MaQGGwTy2Q9"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTbavDgSizUz",
        "outputId": "c0ba2ea7-0b79-46d4-976e-14acd46a1551"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string as str\n",
        "import math\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNdazmeXjrnC"
      },
      "source": [
        "# Get sentences and words after preprocessing\n",
        "\n",
        "def get_tokenized_sents(text):\n",
        "\n",
        "    tokenized_sents = []\n",
        "\n",
        "    # remove frequent words and punctuations\n",
        "    unwanted_words = stopwords.words('english') + list(str.punctuation)\n",
        "\n",
        "    sents = sent_tokenize(text)\n",
        "    for s in sents:\n",
        "        words = word_tokenize(s.lower())\n",
        "        tokenized_sents.append([w for w in words if w not in unwanted_words])\n",
        "\n",
        "    return sents, tokenized_sents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vokHxKuwjzW2"
      },
      "source": [
        "# Get Term frequency\n",
        "\n",
        "def get_tf(tokenized_sents):\n",
        "    tf = {}\n",
        "    for s in tokenized_sents:\n",
        "        for w in s:\n",
        "            tf[w] = tf.get(w,0) + 1\n",
        "\n",
        "    return tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBkZwdMGj2xo"
      },
      "source": [
        "#Correct method\n",
        "def word_overlap(s1, s2):\n",
        "    if len(s1)>len(s2):\n",
        "      return len(set(s1).intersection(s2))\n",
        "    else:\n",
        "      return len(set(s1).intersection(s2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DJrdkCmj7-c"
      },
      "source": [
        "def cosine_sim(s1_vector, s2_vector):\n",
        "    assert len(s1_vector) == len(s2_vector)\n",
        "    num = sum([s1_vector[sid]*s2_vector[sid] for sid in range(len(s1_vector))])\n",
        "    den1 = sum([s1_vector[sid]**2 for sid in range(len(s1_vector))])\n",
        "    den2 = sum([s2_vector[sid]**2 for sid in range(len(s1_vector))])\n",
        "\n",
        "    cosine_sim = num / (math.sqrt(den1)*math.sqrt(den2))\n",
        "    return cosine_sim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VISiebMfkWSg"
      },
      "source": [
        "def get_freqsum_summary(text):\n",
        "\n",
        "    original_sentences, tokenized_sentences = get_tokenized_sents(text)\n",
        "    tf = get_tf(tokenized_sentences)\n",
        "\n",
        "    scores = {}\n",
        "\n",
        "    # Get best sentences based on term frequency\n",
        "    for sid, s in enumerate(tokenized_sentences):\n",
        "        #print(s)\n",
        "        scores[sid] = sum([tf.get(w,0) for w in s])\n",
        "\n",
        "    sorted_scores = sorted(scores.items(), key = lambda x : x[1], reverse = True)\n",
        "    return [original_sentences[s[0]] for s in sorted_scores[0:3]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApMqXMxukbgM"
      },
      "source": [
        "\n",
        "def get_sim_matrix(tokenized_sents, threshold=0.3):\n",
        "    sim_mat = np.zeros((len(tokenized_sents), len(tokenized_sents)))\n",
        "    for s1_id, s1 in enumerate(tokenized_sentences):\n",
        "        for s2_id, s2 in enumerate(tokenized_sentences):\n",
        "            if word_overlap(s1, s2) >= threshold:\n",
        "                sim_mat[s1_id, s2_id] = 1\n",
        "    return sim_mat\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYddv55vkclk"
      },
      "source": [
        "def get_degree_centrality_summary(text, threshold = 0.3):\n",
        "\n",
        "    original_sentences, tokenized_sentences = get_tokenized_sents(text)\n",
        "    tf = get_tf(tokenized_sentences)\n",
        "\n",
        "    sim_mat = get_sim_matrix(tokenized_sentences, threshold)\n",
        "    degree_centrality = sim_mat.sum(axis=1)\n",
        "\n",
        "    scores = {}\n",
        "\n",
        "    for id, d in enumerate(degree_centrality):\n",
        "        scores[id] = d\n",
        "\n",
        "    sorted_scores = sorted(scores.items(), key = lambda x : x[1], reverse = True)\n",
        "\n",
        "    return [original_sentences[s[0]] for s in sorted_scores[0:3]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4zW8aHukigp"
      },
      "source": [
        "def power_method(text, threshold=0.3, lam=0.15, max_num_iter = 100):\n",
        "\n",
        "    delta = 2\n",
        "    epsilon = 0.0001\n",
        "\n",
        "    original_sentences, tokenized_sentences = get_tokenized_sents(text)\n",
        "    tf = get_tf(tokenized_sentences)\n",
        "    num_sents = len(original_sentences)\n",
        "\n",
        "    sim_mat = lam/len(original_sentences) + (1-lam)*get_sim_matrix(tokenized_sentences, threshold)\n",
        "    degree = np.sum(sim_mat, axis=1)\n",
        "\n",
        "    sim_mat_norm = sim_mat/sim_mat.sum(axis=1)\n",
        "\n",
        "    original_scores = np.array([1.0/num_sents for _ in original_sentences])\n",
        "\n",
        "    num_iter = 0\n",
        "\n",
        "    while delta > epsilon:\n",
        "        #print(original_scores)\n",
        "        #print(sim_mat_norm.sum(axis=0))\n",
        "        new_scores = np.matmul(sim_mat_norm, original_scores)\n",
        "        #print(new_scores)\n",
        "\n",
        "        delta = np.mean(abs(new_scores-original_scores))\n",
        "        original_scores = new_scores\n",
        "\n",
        "        print(\"Iteration :{}, Delta: {}\".format(num_iter, delta))\n",
        "\n",
        "        num_iter += 1\n",
        "        if num_iter > max_num_iter:\n",
        "            # Break if required delta not achieved in fixed iterataions\n",
        "            break\n",
        "\n",
        "    scores = {}\n",
        "    for id, d in enumerate(new_scores):\n",
        "        scores[id] = d\n",
        "\n",
        "    sorted_scores = sorted(scores.items(), key = lambda x : x[1], reverse = True)\n",
        "\n",
        "    return [original_sentences[s[0]] for s in sorted_scores[0:3]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "EKa1TfFPkqs8",
        "outputId": "0087a7f0-194b-42aa-a164-22ac8fb26d5b"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-529c2de6-186c-4c23-92a5-9c38ed887acf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-529c2de6-186c-4c23-92a5-9c38ed887acf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving brown.pickle to brown.pickle\n",
            "Saving cambodia.txt to cambodia.txt\n",
            "Saving covid.txt to covid.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfrbkUoi12WG"
      },
      "source": [
        "def clean_text(data):\n",
        "\n",
        "  data=re.sub(\"https*\\S+\", \" \", data)\n",
        "  data=re.sub('[^a-zA-Z]',' ',data)\n",
        "  #data=data.lower()\n",
        "  data=word_tokenize(data)\n",
        "  data=[item for item in data if item not in stop_words]\n",
        "  data=' '.join(data)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9OjpanVkuxn"
      },
      "source": [
        "# Read a custom File\n",
        "with open('/content/covid.txt') as f:\n",
        "    text_covid = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9bV-eohvdaZ"
      },
      "source": [
        "text_covid = clean_text(text_covid)\n",
        "text_covid=text_covid.strip('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYyOcDRcv7Xl",
        "outputId": "246f9746-d499-4985-c7c6-3da825da667d"
      },
      "source": [
        "get_freqsum_summary(text_covid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Coronavirus disease COVID contagious disease caused severe acute respiratory syndrome coronavirus SARS CoV The first case identified Wuhan China December It since spread worldwide leading ongoing pandemic Symptoms COVID variable often include fever cough fatigue breathing difficulties loss smell taste Symptoms begin one fourteen days exposure virus Around one five infected individuals develop symptoms While people mild symptoms people develop acute respiratory distress syndrome ARDS ARDS precipitated cytokine storms multi organ failure septic shock blood clots Longer term damage organs particular lungs heart observed There concern significant number patients recovered acute phase disease continue experience range effects known long COVID months afterwards These effects include severe fatigue memory loss cognitive issues low grade fever muscle weakness breathlessness The virus causes COVID spreads mainly infected person close contact another person Small droplets aerosols containing virus spread infected person nose mouth breathe cough sneeze sing speak Other people infected virus gets mouth nose eyes The virus may also spread via contaminated surfaces although thought main route transmission The exact route transmission rarely proven conclusively infection mainly happens people near long enough It spread early two days infected persons show symptoms individuals never experience symptoms People remain infectious ten days moderate cases two weeks severe cases Various testing methods developed diagnose disease The standard diagnosis method real time reverse transcription polymerase chain reaction rRT PCR nasopharyngeal swab Preventive measures include physical social distancing quarantining ventilation indoor spaces covering coughs sneezes hand washing keeping unwashed hands away face The use face masks coverings recommended public settings minimise risk transmissions Several vaccines developed various countries initiated mass vaccination campaigns']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SD9SPtw0icq"
      },
      "source": [
        "USING COSINE_SIM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miFpJ-y8yFR7"
      },
      "source": [
        "with open('/content/cambodia.txt') as f:\n",
        "    text_camb = f.read()\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lymXf-RZ0QVk"
      },
      "source": [
        "text_camb_list = []\n",
        "\n",
        "for word in text_camb.split(' '):\n",
        "  text_camb_list.append(word)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10F0Myxv2z0U"
      },
      "source": [
        "text_covid_list = []\n",
        "\n",
        "for word in text_covid.split(' '):\n",
        "  text_covid_list.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qpxw9MY0ngh"
      },
      "source": [
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "def get_sim_matrix_cosine(tokenized_sents, threshold=0.3):\n",
        "    sim_mat = np.zeros((len(tokenized_sents), len(tokenized_sents)))\n",
        "    for s1_id, s1 in enumerate(tokenized_sents):\n",
        "        for s2_id, s2 in enumerate(tokenized_sents):\n",
        "            r1 = tfidf.transform(s1)\n",
        "            r2 = tfidf.transform(s2)\n",
        "            if cosine_sim(r1, r2) >= threshold:\n",
        "                sim_mat[s1_id, s2_id] = 1\n",
        "    return sim_mat\n",
        "\n",
        "#text = text.strip('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3irREzNxdHw"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "vectorizer = TfidfVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPl1JxORxlPh"
      },
      "source": [
        "X = vectorizer.fit_transform(text_camb_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okdGeQ2dyrHJ",
        "outputId": "777aa174-b85d-4404-d2db-39c6d99ecd5c"
      },
      "source": [
        "query_vec = vectorizer.transform(text_covid_list)\n",
        "results = cosine_similarity(X,query_vec).reshape((-1,))\n",
        "print(\"Array is ,\",results)\n",
        "print(\"total is \",np.sum(results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Array is , [0. 0. 0. ... 0. 0. 0.]\n",
            "total is  2005.54510739295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW60UD5N4cr-"
      },
      "source": [
        "HOW COVID SPREADS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GrGO1FT61jN"
      },
      "source": [
        "using similarity_index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTdKktzA7Vd0"
      },
      "source": [
        "def text_list2(text):\n",
        "  text_list = []\n",
        "\n",
        "  for word in text.split('.'):\n",
        "    text_list.append(word)\n",
        "  return text_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "Cvw3oLflB5gd",
        "outputId": "52c9de95-9e0d-4e95-d266-a36362ede4e0"
      },
      "source": [
        "def get_freqsum_summary(text,query):\n",
        "    #print(text)\n",
        "    text_list=text_list2(text)\n",
        "    original_sentences, tokenized_sentences = get_tokenized_sents(text)\n",
        "    tf = get_tf(tokenized_sentences)\n",
        "\n",
        "    original_sentences2, tokenized_sentences2 = get_tokenized_sents(query)\n",
        "    qtf = get_tf(tokenized_sentences2)\n",
        "\n",
        "    scores = {}\n",
        "\n",
        "    #print(tf)\n",
        "    # Get best sentences based on term frequency\n",
        "    for sid, s in enumerate(tokenized_sentences):\n",
        "\n",
        "        scores[sid] = sum([tf.get(w,0) for w in s])\n",
        "\n",
        "\n",
        "    sorted_scores = sorted(scores.items(), key = lambda x : x[1], reverse = True)\n",
        "    overlap_score=0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for s in text_list:\n",
        "\n",
        "      overlap_score2=word_overlap(query,s)\n",
        "      if overlap_score<overlap_score2:\n",
        "        overlap_score=overlap_score2\n",
        "        sent=s\n",
        "    return sent\n",
        "\n",
        "with open('/content/covid.txt') as f:\n",
        "    text_covid = f.read()\n",
        "get_freqsum_summary(text_covid,\"How covid spreads\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Coronavirus disease 2019 (COVID-19) is a contagious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The first case was identified in Wuhan, China, in December 2019. It has since spread worldwide, leading to an ongoing pandemic.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    }
  ]
}